{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `About Deepl-Learning`\n",
    "- Source Link `https://towardsdatascience.com/deep-learning-with-python-neural-networks-complete-tutorial-6b53c0b06af0`\n",
    "- TensorFlow(by Goolge) - Production Ready \n",
    "- PyTorch(by Facebook) - Good for rapid prototypes - Easy to learn\n",
    "- Both levarage power of NVIDIA GPU's. It is useful for processing big datasets(corpus of text & gallery of images)\n",
    "- TensorFlow a higher-level module way more user-friendly than pure TensorFlow\n",
    "- Enabling GPU support, python syntax will be translatedinto GUDA by machine and processed by GPU's, so model shall run incredibly faster\n",
    "- Artificial Neural Networks\n",
    "  - ANN are made of layers(input & output dimension)\n",
    "  - Neurons known as \"nodes\". It is computational unit that connects the weighted inputs through an `activation function - helps neuron to switch on/off`\n",
    "  - `Weights` are randomly initialised and optimised during training to minimize a loss function\n",
    "  - Input Data - Matrix of 3 features(shape N*3)\n",
    "  - `Input Layer` - Takes 3 numbers as input and passes the same 3 numbers to next layers\n",
    "  - `Hidden Layer` - represent intermediary nodes, they do serveral transformations to improve accuracy of final result & output is defined by number of neurons\n",
    "  - `Output Layer` - returns final output if Neural Network. For `Binary classification - o/p layer contains 1 neuron to return only 1 number`, for `Multiclass Classification(5 classes) - o/p layer contains 5 neurons`\n",
    "  - Simplest form of ANN is `Perceptron`. A model with 1 layer only - very similar to `Linear Regression`\n",
    "  - Asking what happens inside a Perceptron is equivalent to asking what happens inside a single node of a multi-layer Neural Network\n",
    "  - Data should always be scaled before being fed into Neural Network\n",
    "  - Just like in every other ML use case, we are going to train a model to predict the target using the features row by row. Let's start with the first row.\n",
    "  - 'Training Model' means searching best parameters in a mathematical formula that minimize the error of your predictions\n",
    "    - In `Linear Regression Model` you have to find best weights\n",
    "    - In `Tree-Based Model(Random Forest)` you have to find best splitting points\n",
    "  - Weights are randomly initialised then adjusted as the learning proceeds\n",
    "  - In `linear model - Σ(xi * wights + bias)` - we can use Linear Regression\n",
    "  - In `non-linear model - f(Σ(xi * wights ) + bias)` - we can use Deep Learning, where `activation functions` evaluates the regression formula output & helps in learning weights\n",
    "  - Activation function defines output of that node. We can use out of many activation functions or custom functions following - `Keras - Activation Function / Wiki - Activation Function`\n",
    "  - Ex:- Binary step - Activation Function returns 1 or 0 only\n",
    "  - Perceptron - Single_Layer Network take input 1 row(3 column values) do regression formula and gets output goes into activation function and defines output(or give further output). Then that output compared with target & calculating error & optimizing weights, reiterating the whole process again and again. \n",
    "- Deep Neural Networks   \n",
    "  - A Neural Network can be called as `Deep Neural Network` when it has atleast 2 hiddem layers.\n",
    "  - Imagining replicating neuron process 3 times(i.e., 3 neurons in 1st layer) simaltaneously. Since each node(weighted sum & activation function) returns a value. We would have `1st hidden layer with 3 outputs` \n",
    "  - Now using those 3 outputs as inputs for the 2nd hidden layer, which returns 3 new numbers. Finally we shall add an output layer(1 node only) to get final prediction of our model.\n",
    "  - Remember that layers can have a diff. number of neurons and a diff. activation func. and in each node, weights are trained to optimize the final result.\n",
    "  - That's why the more layers you add, the bigger the no. of trainable parameters gets.\n",
    "  - `Bias`: inside each neuron, the linear combination of inputs and weights includes also a bias, similar to the constant in a linear equation, there full formula of a neuron is:\n",
    "    - `f(Σ(xi * wights ) + bias)`\n",
    "  - `Backpropagation:` During Training, the model learns by propagating the error back into the nodes and updating the parameters(weights and biases) to minimize the loss.\n",
    "  - `Gradient Descent:` Optimization algorithm used to train Neural Network which finds local minimum of the loss function by taking repeated steps in direction of steepest descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Import Libraries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "import shap\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from io import StringIO\n",
    "from IPython.display import display, HTML\n",
    "from tensorflow.keras import models, layers, utils, backend as bk\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Sample Data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Sell</th>\n",
       "      <th>Revenue_Generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>2800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31</td>\n",
       "      <td>2900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "companya_sales_data = \"\"\"\n",
    "Product_Sell,Revenue_Generation\n",
    "10,1000\n",
    "15,1400\n",
    "18,1800\n",
    "22,2400\n",
    "26,2600\n",
    "30,2800\n",
    "5,700\n",
    "31,2900\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_table(StringIO(companya_sales_data), sep=\",\")\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Train & Test Split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape :  (5,)\n",
      "X_test.shape :  (3,)\n",
      "X_train -->  [26 10 30 22  5]\n",
      "X_test -->  [31 15 18]\n",
      "y_train -->  [2600 1000 2800 2400  700]\n",
      "y_test -->  [2900 1400 1800]\n"
     ]
    }
   ],
   "source": [
    "X = df[\"Product_Sell\"].values\n",
    "y = df[\"Revenue_Generation\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)\n",
    "print(\"X_train.shape : \", X_train.shape); print(\"X_test.shape : \", X_test.shape)\n",
    "\n",
    "print(\"X_train --> \", X_train)\n",
    "print(\"X_test --> \", X_test)\n",
    "print(\"y_train --> \", y_train)\n",
    "print(\"y_test --> \", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Artifical Neural Network : Perceptron / 1 Dense Layer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Model - Fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Perceptron\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential(name=\"Perceptron\", layers=[\n",
    "    ##### Fully Connected Layer\n",
    "    layers.Dense(\n",
    "        name = \"dense\", \n",
    "        input_dim = 1, # with 3 features as the input\n",
    "        activation = \"relu\", # [rectified linear unit] / linear /  [f(x) = x ]\n",
    "        units = 1, # 1 Node because we want 1 output\n",
    "    )\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Observation`\n",
    "- 2 params (1 Weight & 1 Bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Activation Function`\n",
    "  - The activation function used in the hidden layers is a `rectified linear unit, or ReLU`. \n",
    "  - It is the most widely used activation function because of its advantages of being nonlinear, as well as the ability to not activate all the neurons at the same time. \n",
    "  - In simple terms, this means that at a time, only a few neurons are activated, making the network sparse and very efficient.\n",
    "- `optimizer`\n",
    "  - optimizer and the loss measure for training. The `Mean Squared Error : Loss Measure(Loss Function)` and the `Adam Optimizer : minimization algorithm(minimises loss value)`. \n",
    "  - The main advantage of the \"adam\" optimizer is that we don't need to specify the learning rate as is the case with gradient descent; thereby saving us the task of optimizing the learning rate for our model.  \n",
    "- `epochs`\n",
    "  - Represents the number of training iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Without Data Scaling`\n",
    "- `epochs=20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 510ms/step - loss: 4324576.0000 - mean_squared_error: 4324576.0000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4324485.5000 - mean_squared_error: 4324485.5000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4324395.0000 - mean_squared_error: 4324395.0000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4324305.0000 - mean_squared_error: 4324305.0000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4324214.5000 - mean_squared_error: 4324214.5000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4324123.5000 - mean_squared_error: 4324123.5000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4324033.0000 - mean_squared_error: 4324033.0000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4323942.5000 - mean_squared_error: 4323942.5000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4323852.5000 - mean_squared_error: 4323852.5000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4323762.0000 - mean_squared_error: 4323762.0000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4323671.5000 - mean_squared_error: 4323671.5000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4323581.0000 - mean_squared_error: 4323581.0000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4323490.5000 - mean_squared_error: 4323490.5000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4323400.5000 - mean_squared_error: 4323400.5000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4323309.0000 - mean_squared_error: 4323309.0000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4323219.0000 - mean_squared_error: 4323219.0000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4323129.0000 - mean_squared_error: 4323129.0000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 4323038.5000 - mean_squared_error: 4323038.5000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4322947.5000 - mean_squared_error: 4322947.5000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4322857.0000 - mean_squared_error: 4322857.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff7e9bf5710>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "model.fit(X_train, y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Model Evaluation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on Train Data : \n",
      "----------------------------------------\n",
      "y_train -->  [2600 1000 2800 2400  700]\n",
      "pred_train -->  [[14.125604629516602], [5.445232391357422], [16.295698165893555], [11.955511093139648], [2.7326161861419678]]\n",
      "MSE -->  2079.126427131573\n",
      "----------------------------------------\n",
      "Predictions on Test Data : \n",
      "----------------------------------------\n",
      "y_test -->  [2900 1400 1800]\n",
      "pred -->  [[16.83822250366211], [8.157848358154297], [9.785418510437012]]\n",
      "MSE -->  2117.7594022992553\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions on Train Data : \")\n",
    "print(\"--\"*20)\n",
    "pred_train = model.predict(X_train)\n",
    "print(\"y_train --> \", y_train)\n",
    "print(\"pred_train --> \", pred_train.tolist())\n",
    "print(\"MSE --> \", np.sqrt(mean_squared_error(y_train,pred_train)))\n",
    "print(\"--\"*20)\n",
    "print(\"Predictions on Test Data : \")\n",
    "print(\"--\"*20)\n",
    "pred_test = model.predict(X_test)\n",
    "print(\"y_test --> \", y_test)\n",
    "print(\"pred --> \", pred_test.tolist())\n",
    "print(\"MSE --> \", np.sqrt(mean_squared_error(y_test,pred_test))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Observation`\n",
    "- Without data scaling ANN. doesn't make correct predictions. As it is not classic Linear Regression to check relation between indep. & dep. variables. \n",
    "- So variables(indep. & dep.) should be on same scale  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Data Scaling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Independent Data : \n",
      "----------------------------------------\n",
      "Before Scaling : X_train -->  [26 10 30 22  5]\n",
      "Before Scaling : X_test -->  [31 15 18]\n",
      "After Scaling : X_train_scaled -->  [[0.84]\n",
      " [0.2 ]\n",
      " [1.  ]\n",
      " [0.68]\n",
      " [0.  ]]\n",
      "After Scaling : X_test_scaled -->  [[1.04]\n",
      " [0.4 ]\n",
      " [0.52]]\n",
      "Transform Dependent Data : \n",
      "----------------------------------------\n",
      "Before Scaling : y_train -->  [2600 1000 2800 2400  700]\n",
      "After Scaling : y_train_scaled -->  [[0.9047619 ]\n",
      " [0.14285714]\n",
      " [1.        ]\n",
      " [0.80952381]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "##### Transform Independent Data #####\n",
    "print(\"Transform Independent Data : \")\n",
    "print(\"--\"*20)\n",
    "print(\"Before Scaling : X_train --> \", X_train)\n",
    "print(\"Before Scaling : X_test --> \", X_test)\n",
    "# Initialise MinMaxScaler\n",
    "x_scaler = MinMaxScaler()\n",
    "X_train_arr = np.array(X_train).reshape(-1, 1)\n",
    "X_test_arr = np.array(X_test).reshape(-1, 1)\n",
    "x_scaler.fit(X_train_arr)\n",
    "X_train_scaled = x_scaler.transform(X_train_arr)\n",
    "X_test_scaled = x_scaler.transform(X_test_arr)\n",
    "print(\"After Scaling : X_train_scaled --> \", X_train_scaled)\n",
    "print(\"After Scaling : X_test_scaled --> \", X_test_scaled)\n",
    "##### Transform Dependent Data #####\n",
    "print(\"Transform Dependent Data : \")\n",
    "print(\"--\"*20)\n",
    "print(\"Before Scaling : y_train --> \", y_train)\n",
    "# Initialise MinMaxScaler\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train_arr = np.array(y_train).reshape(-1, 1)\n",
    "y_scaler.fit(y_train_arr)\n",
    "y_train_scaled = y_scaler.transform(y_train_arr)\n",
    "print(\"After Scaling : y_train_scaled --> \", y_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `With Data Scaling`\n",
    "- `epochs=30`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1/1 [==============================] - 1s 517ms/step - loss: 4315566.0000 - mean_squared_error: 4315566.0000\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4315475.5000 - mean_squared_error: 4315475.5000\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4315385.5000 - mean_squared_error: 4315385.5000\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4315295.0000 - mean_squared_error: 4315295.0000\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4315203.5000 - mean_squared_error: 4315203.5000\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4315113.5000 - mean_squared_error: 4315113.5000\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4315023.0000 - mean_squared_error: 4315023.0000\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4314933.0000 - mean_squared_error: 4314933.0000\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4314843.0000 - mean_squared_error: 4314843.0000\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4314753.0000 - mean_squared_error: 4314753.0000\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4314662.0000 - mean_squared_error: 4314662.0000\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4314571.5000 - mean_squared_error: 4314571.5000\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4314481.5000 - mean_squared_error: 4314481.5000\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4314391.0000 - mean_squared_error: 4314391.0000\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4314301.0000 - mean_squared_error: 4314301.0000\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4314210.5000 - mean_squared_error: 4314210.5000\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4314119.5000 - mean_squared_error: 4314119.5000\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4314029.5000 - mean_squared_error: 4314029.5000\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4313939.0000 - mean_squared_error: 4313939.0000\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4313849.0000 - mean_squared_error: 4313849.0000\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4313758.0000 - mean_squared_error: 4313758.0000\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4313668.0000 - mean_squared_error: 4313668.0000\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4313577.5000 - mean_squared_error: 4313577.5000\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4313487.0000 - mean_squared_error: 4313487.0000\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4313397.0000 - mean_squared_error: 4313397.0000\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4313306.5000 - mean_squared_error: 4313306.5000\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4313216.0000 - mean_squared_error: 4313216.0000\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4313126.0000 - mean_squared_error: 4313126.0000\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4313035.5000 - mean_squared_error: 4313035.5000\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4312945.0000 - mean_squared_error: 4312945.0000\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4312854.5000 - mean_squared_error: 4312854.5000\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4312764.5000 - mean_squared_error: 4312764.5000\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4312674.0000 - mean_squared_error: 4312674.0000\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4312583.5000 - mean_squared_error: 4312583.5000\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4312493.0000 - mean_squared_error: 4312493.0000\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4312403.0000 - mean_squared_error: 4312403.0000\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4312312.5000 - mean_squared_error: 4312312.5000\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4312222.5000 - mean_squared_error: 4312222.5000\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4312132.0000 - mean_squared_error: 4312132.0000\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4312041.5000 - mean_squared_error: 4312041.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff7ead94750>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "model.fit(X_train, y_train, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Model Evaluation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Predictions : \n",
      "----------------------------------------\n",
      "y_train -->  [[2600.]\n",
      " [1000.]\n",
      " [2800.]\n",
      " [2400.]\n",
      " [ 700.]]\n",
      "train_predictions -->  [[2161.0076 ]\n",
      " [1271.066  ]\n",
      " [2383.4927 ]\n",
      " [1938.522  ]\n",
      " [ 992.95935]]\n",
      "MSE -->  384.30629842480687\n",
      "Test Predictions : \n",
      "----------------------------------------\n",
      "y_test -->  [2900 1400 1800]\n",
      "test_predictions -->  [[2439.1143]\n",
      " [1549.1727]\n",
      " [1716.0366]]\n",
      "MSE -->  283.8532599620366\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Predictions : \")\n",
    "print(\"--\"*20)\n",
    "pred = model.predict(X_train_scaled)\n",
    "#invert normalize\n",
    "y_train = y_scaler.inverse_transform(y_train_scaled)\n",
    "train_predictions = y_scaler.inverse_transform(pred) \n",
    "print(\"y_train --> \", y_train)\n",
    "print(\"train_predictions --> \", train_predictions)\n",
    "print(\"MSE --> \", np.sqrt(mean_squared_error(y_train,train_predictions)))\n",
    "\n",
    "print(\"Test Predictions : \")\n",
    "print(\"--\"*20)\n",
    "pred = model.predict(X_test_scaled)\n",
    "#invert normalize\n",
    "test_predictions = y_scaler.inverse_transform(pred) \n",
    "print(\"y_test --> \", y_test)\n",
    "print(\"test_predictions --> \", test_predictions)\n",
    "print(\"MSE --> \", np.sqrt(mean_squared_error(y_test,test_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Observation`\n",
    "- After data scaling & increased training iterations i.e., epochs. With single perceptron, ANN. is able to make correct predictions. \n",
    "- As data(indep. & dep.) is some what linearly related. So with single perceptron & less epoch we were able to get good predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Deep Learning Neural Network - 2 Layers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Model - Fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 5)                 10        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16\n",
      "Trainable params: 16\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(5, input_dim=1, activation= \"relu\"))\n",
    "model.add(layers.Dense(1))\n",
    "model.summary() #Print model Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Observation`\n",
    "- 1st layer has 1 input(indep. variable value)\n",
    "- 5 Nodes in 1st layer\n",
    "- 1 Node in 2nd layer\n",
    "- Each node has 2 parameters in this case i.e., 1 indep.var coeff + bias\n",
    "- So 1st layer has = 5 * 10 = Total 10 trainable parameters\n",
    "- 2nd layer has 5 inputs(i.e., 1st layer ouput). Hence, 5 indep.var coeff + bias = 6 trainable parameters\n",
    "- On total, 10 + 6 = `16 trainable parameters` in this deepl learning neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `With Data Scaling`\n",
    "- `epochs=20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 584ms/step - loss: 0.9033 - mean_squared_error: 0.9033\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8919 - mean_squared_error: 0.8919\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.8806 - mean_squared_error: 0.8806\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8694 - mean_squared_error: 0.8694\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8583 - mean_squared_error: 0.8583\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8472 - mean_squared_error: 0.8472\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8363 - mean_squared_error: 0.8363\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8254 - mean_squared_error: 0.8254\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8147 - mean_squared_error: 0.8147\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8040 - mean_squared_error: 0.8040\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7934 - mean_squared_error: 0.7934\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7829 - mean_squared_error: 0.7829\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7726 - mean_squared_error: 0.7726\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7623 - mean_squared_error: 0.7623\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7521 - mean_squared_error: 0.7521\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7420 - mean_squared_error: 0.7420\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7320 - mean_squared_error: 0.7320\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7221 - mean_squared_error: 0.7221\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7123 - mean_squared_error: 0.7123\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7027 - mean_squared_error: 0.7027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff7eb117d10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss= \"mean_squared_error\" , optimizer=\"adam\", metrics=[\"mean_squared_error\"])\n",
    "model.fit(X_train_scaled, y_train_scaled, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Model Evaluation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Predictions : \n",
      "----------------------------------------\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff7eb19db00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "y_train -->  [[2600.]\n",
      " [1000.]\n",
      " [2800.]\n",
      " [2400.]\n",
      " [ 700.]]\n",
      "train_predictions -->  [[361.43274]\n",
      " [736.11523]\n",
      " [267.7621 ]\n",
      " [455.1032 ]\n",
      " [781.4446 ]]\n",
      "MSE -->  1748.273671390504\n",
      "Test Predictions : \n",
      "----------------------------------------\n",
      "y_test -->  [2900 1400 1800]\n",
      "test_predictions -->  [[244.34433]\n",
      " [619.0269 ]\n",
      " [548.774  ]]\n",
      "MSE -->  1753.852191766989\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Predictions : \")\n",
    "print(\"--\"*20)\n",
    "pred = model.predict(X_train_scaled)\n",
    "#invert normalize\n",
    "y_train = y_scaler.inverse_transform(y_train_scaled)\n",
    "train_predictions = y_scaler.inverse_transform(pred) \n",
    "print(\"y_train --> \", y_train)\n",
    "print(\"train_predictions --> \", train_predictions)\n",
    "print(\"MSE --> \", np.sqrt(mean_squared_error(y_train,train_predictions)))\n",
    "\n",
    "print(\"Test Predictions : \")\n",
    "print(\"--\"*20)\n",
    "pred = model.predict(X_test_scaled)\n",
    "#invert normalize\n",
    "test_predictions = y_scaler.inverse_transform(pred) \n",
    "print(\"y_test --> \", y_test)\n",
    "print(\"test_predictions --> \", test_predictions)\n",
    "print(\"MSE --> \", np.sqrt(mean_squared_error(y_test,test_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Observation`\n",
    "- In single perceptron(single layer-single node-single activation function), predictions were good with increasing epochs. \n",
    "- But in deep learning i.e., dense layers as the training parameters increase upto 16. Traning become complex even on less data i.e., too linearly related data.\n",
    "- This DL network give us good results on high epoch numbers. So as to train 16 parameters in correct direction.\n",
    "- May be DL network used in multi linear regression problem & ANN(Single perceptron) is used in Simple Linear Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Reference Links`\n",
    "- https://www.h2kinfosys.com/blog/linear-regression-with-keras-on-tensorflow/\n",
    "- https://www.pluralsight.com/guides/regression-keras\n",
    "- https://datascienceplus.com/keras-regression-based-neural-networks/\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
